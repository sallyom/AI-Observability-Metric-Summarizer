# Makefile for RAG Deployment
# Replaces the original deploy.sh script with additional uninstall functionality
ifeq ($(NAMESPACE),)
ifeq (,$(filter depend install-ingestion-pipeline list-models% help,$(MAKECMDGOALS)))
$(error NAMESPACE is not set)
endif
endif

MAKEFLAGS += --no-print-directory

# Default values
POSTGRES_USER ?= postgres
POSTGRES_PASSWORD ?= rag_password
POSTGRES_DBNAME ?= rag_blueprint
MINIO_USER ?= minio_rag_user
MINIO_PASSWORD ?= minio_rag_password
HF_TOKEN ?= $(shell bash -c 'read -r -p "Enter Hugging Face Token: " HF_TOKEN; echo $$HF_TOKEN')
RAG_CHART := rag
METRIC_MCP_RELEASE_NAME ?= metric-mcp
METRIC_MCP_CHART_PATH ?= metric-mcp
METRIC_UI_RELEASE_NAME ?= ui
METRIC_UI_CHART_PATH ?= ui
TOLERATIONS_TEMPLATE=[{"key":"$(1)","effect":"NoSchedule","operator":"Exists"}]

# If using extra tools that require API Keys set the LLAMA_STACK_ENV variable with key, value pairs
# LLAMA_STACK_ENV ?= {Key1: "Value1", Key2: "Value2" etc.}

#ingestion pipeline configuration
SOURCE ?= S3
EMBEDDING_MODEL ?= all-MiniLM-L6-v2
INGESTION_PIPELINE_NAME ?= demo-rag-vector-db
INGESTION_PIPELINE_VERSION ?= 1.0
ACCESS_KEY_ID ?= $(MINIO_USER)
SECRET_ACCESS_KEY ?= $(MINIO_PASSWORD)
BUCKET_NAME ?= documents
ENDPOINT_URL ?= http://minio:9000
REGION ?= us-east-1
# PDF file path variable for upload-pdf target
PDF_DIR = ../../notebooks
S3_TEMPLATE={"access_key_id":"$(1)","secret_access_key":"$(2)","bucket_name":"$(3)","endpoint_url":"$(4)","region":"$(5)"}

# CUSTOM VALUES (full path) FOR EXTRA INGESTION PIPELINE
# CUSTOM_INGESTION_PIPELINE_VALUES = ~/my-values.yaml
# CUSTOM_INGESTION_PIPELINE_NAME ?= my-pipeline


helm_llm_service_args = \
    --set llm-service.secret.hf_token=$(HF_TOKEN) \
    $(if $(DEVICE),--set llm-service.device='$(DEVICE)',) \
    $(if $(LLM),--set global.models.$(LLM).enabled=true,) \
    $(if $(SAFETY),--set global.models.$(SAFETY).enabled=true,) \
    $(if $(LLM_TOLERATION),--set-json global.models.$(LLM).tolerations='$(call TOLERATIONS_TEMPLATE,$(LLM_TOLERATION))',) \
    $(if $(SAFETY_TOLERATION),--set-json global.models.$(SAFETY).tolerations='$(call TOLERATIONS_TEMPLATE,$(SAFETY_TOLERATION))',) \
    $(if $(RAW_DEPLOYMENT),--set llm-service.rawDeploymentMode=$(RAW_DEPLOYMENT),)


# Default target
.PHONY: help
help:
	@echo "Available targets:"
	@echo "  list-models   - List available models"
	@echo "  install       - Install the RAG deployment (creates namespace, secrets, and deploys Helm chart)"
	@echo "  install-ingestion-pipeline - Install extra ingestion pipelines (must be on the same namespace as the RAG deployment)"
	@echo "  uninstall     - Uninstall the RAG deployment and clean up resources"
	@echo "  status        - Check status of the deployment"
	@echo ""
	@echo "Minikube-specific targets:"
	@echo "  install-minikube-mcp     - Install metric-mcp for Minikube"
	@echo "  install-minikube-ui      - Install metric-ui for Minikube"
	@echo "  install-minikube         - Install both metric-mcp and metric-ui for Minikube"
	@echo "  uninstall-minikube       - Uninstall Minikube deployments"
	@echo "  status-minikube          - Check Minikube deployment status"
	@echo ""
	@echo "EC2-specific targets:"
	@echo "  install-ec2-mcp          - Install metric-mcp for EC2"
	@echo "  install-ec2-ui           - Install metric-ui for EC2"
	@echo "  install-ec2              - Install both metric-mcp and metric-ui for EC2"
	@echo "  configure-ec2-ingress    - Remove host restrictions for EC2 access"
	@echo ""
	@echo "Configuration options (set via environment variables or make arguments):"
	@echo "  NAMESPACE                - Target namespace (default: llama-stack-rag)"
	@echo "  HF_TOKEN                 - Hugging Face Token (will prompt if not provided)"
	@echo "  DEVICE                	  - Deploy models on cpu or gpu (default)"
	@echo "  {SAFETY,LLM}             - Model id as defined in values (eg. llama-3-2-1b-instruct)"
	@echo "  {SAFETY,LLM}_URL         - Model URL"
	@echo "  {SAFETY,LLM}_API_TOKEN   - Model API token for remote models"
	@echo "  {SAFETY,LLM}_TOLERATION  - Model pod toleration"
	@echo "  CUSTOM_INGESTION_PIPELINE_VALUES - Custom ingestion pipeline values (full path)"
	@echo "  CUSTOM_INGESTION_PIPELINE_NAME   - Custom ingestion pipeline name"
	@echo "  LLAMA_STACK_ENV          - List of environment variables for llama-stack (eg. {TAVILY_SEARCH_API_KEY: \"<Your Tavily Search API Key here>\"})"
	@echo ""
	@echo "Minikube-specific configuration:"
	@echo "  PROMETHEUS_URL           - Your Prometheus service URL"
	@echo "  LLM_URL                  - Your VLLM/LLM service URL"
	@echo "  LLM_API_TOKEN            - API token for your LLM service (if required)"


# Create namespace and deploy
namespace:
	@oc create namespace $(NAMESPACE) &> /dev/null && oc label namespace $(NAMESPACE) modelmesh-enabled=false ||:
	@oc project $(NAMESPACE) &> /dev/null ||:

.PHONY: depend
depend:
	@echo "Updating Helm dependencies"
	@helm dependency update $(RAG_CHART) &> /dev/null

.PHONY: list-models
list-models: depend
	@helm template dummy-release $(RAG_CHART) --set llm-service._debugListModels=true | grep ^model:

.PHONY: install-metric-mcp
install-metric-mcp: namespace
	@echo "Extracting LLM service URL"
	LLM_URL=$$(oc get inferenceservice llama-3-2-3b-instruct -n $(NAMESPACE) -o jsonpath='{.status.url}'); \
	echo "Detected LLM_URL=$$LLM_URL"; \
	helm upgrade --install $(METRIC_MCP_RELEASE_NAME) $(METRIC_MCP_CHART_PATH) \
		-n $(NAMESPACE) \
		--set-json llm.url="\"$$LLM_URL\"" \
		--set-json listModels.modelId.enabledModelIds='$(MODELS_JSON)'

.PHONY: install-metric-ui
install-metric-ui: namespace
	@echo "Deploying Metric UI"
	helm upgrade --install $(METRIC_UI_RELEASE_NAME) $(METRIC_UI_CHART_PATH) -n $(NAMESPACE)

.PHONY: install-rag
install-rag: namespace
	@$(eval LLM_SERVICE_ARGS := $(call helm_llm_service_args))
	@echo "Installing $(RAG_CHART) helm chart"
	@helm -n $(NAMESPACE) upgrade --install $(RAG_CHART) $(RAG_CHART) -n $(NAMESPACE) \
		$(LLM_SERVICE_ARGS)
	@echo "Waiting for model services to deploy. It will take around 10-15 minutes depending on the size of the model..."
	@oc wait -n $(NAMESPACE) --for=condition=Ready --timeout=60m inferenceservice --all ||:
	@echo "$(RAG_CHART) installed successfully"

.PHONY: install
install: namespace depend install-rag install-metric-mcp install-metric-ui delete-jobs
	@echo "Waiting for deployment to be ready..."
	@$(MAKE) wait

# Uninstall the deployment and clean up
.PHONY: uninstall
uninstall:
	@echo "Uninstalling $(RAG_CHART) helm chart"
	@helm -n $(NAMESPACE) uninstall $(RAG_CHART)
	@echo "Removing pgvector and minio PVCs from $(NAMESPACE)"
	@oc get pvc -n $(NAMESPACE) -o custom-columns=NAME:.metadata.name | grep -E '^(pg|minio)-data' | xargs -I {} oc delete pvc -n $(NAMESPACE) {} ||:
	@echo "Deleting remaining pods in namespace $(NAMESPACE)"
	@oc delete pods -n $(NAMESPACE) --all
	@echo "Checking for any remaining resources in namespace $(NAMESPACE)..."
	@echo "If you want to completely remove the namespace, run: oc delete project $(NAMESPACE)"
	@echo "Remaining resources in namespace $(NAMESPACE):"
	@$(MAKE) status

# Install extra ingestion pipelines
.PHONY: install-ingestion-pipeline
install-ingestion-pipeline:
	helm -n $(NAMESPACE) install $(CUSTOM_INGESTION_PIPELINE_NAME) rag/charts/ingestion-pipeline-0.1.0.tgz -f $(CUSTOM_INGESTION_PIPELINE_VALUES)

# Delete all jobs in the namespace
.PHONY: delete-jobs
delete-jobs:
	@echo "Deleting all jobs in namespace $(NAMESPACE)"
	@oc delete jobs -n $(NAMESPACE) --all ||:
	@echo "Job deletion completed"

# Check deployment status
.PHONY: status
status:
	@echo "Listing pods..."
	oc get pods -n $(NAMESPACE) || true

	@echo "Listing services..."
	oc get svc -n $(NAMESPACE) || true

	@echo "Listing routes..."
	oc get routes -n $(NAMESPACE) || true

	@echo "Listing secrets..."
	oc get secrets -n $(NAMESPACE) | grep huggingface-secret || true

	@echo "Listing pvcs..."
	oc get pvc -n $(NAMESPACE) || true

# Minikube-specific targets
.PHONY: namespace-minikube
namespace-minikube:
	@kubectl create namespace $(NAMESPACE) &> /dev/null || true
	@kubectl config set-context --current --namespace=$(NAMESPACE) &> /dev/null || true

.PHONY: install-minikube-mcp
install-minikube-mcp: namespace-minikube
	@echo "Installing metric-mcp for Minikube"
	@if [ -z "$(PROMETHEUS_URL)" ] || [ -z "$(LLM_URL)" ]; then \
		echo "Error: PROMETHEUS_URL and LLM_URL must be set"; \
		echo "Example: make install-minikube-mcp PROMETHEUS_URL=http://prometheus.monitoring.svc.cluster.local:9090 LLM_URL=http://vllm-service.default.svc.cluster.local:8000"; \
		exit 1; \
	fi
	@helm upgrade --install $(METRIC_MCP_RELEASE_NAME) $(METRIC_MCP_CHART_PATH) \
		-n $(NAMESPACE) \
		-f $(METRIC_MCP_CHART_PATH)/values-minikube.yaml \
		--set config.prometheusUrl="$(PROMETHEUS_URL)" \
		--set llm.url="$(LLM_URL)" \
		$(if $(LLM_API_TOKEN),--set llm.apiToken="$(LLM_API_TOKEN)",)

.PHONY: install-minikube-ui
install-minikube-ui: namespace-minikube
	@echo "Installing metric-ui for Minikube"
	@if [ -z "$(PROMETHEUS_URL)" ] || [ -z "$(LLM_URL)" ]; then \
		echo "Error: PROMETHEUS_URL and LLM_URL must be set"; \
		echo "Example: make install-minikube-ui PROMETHEUS_URL=http://prometheus.monitoring.svc.cluster.local:9090 LLM_URL=http://vllm-service.default.svc.cluster.local:8000"; \
		exit 1; \
	fi
	@helm upgrade --install $(METRIC_UI_RELEASE_NAME) $(METRIC_UI_CHART_PATH) \
		-n $(NAMESPACE) \
		-f $(METRIC_UI_CHART_PATH)/values-minikube.yaml \
		--set env.PROMETHEUS_URL="$(PROMETHEUS_URL)" \
		--set env.LLM_URL="$(LLM_URL)" \
		$(if $(LLM_API_TOKEN),--set env.LLM_API_TOKEN="$(LLM_API_TOKEN)",)

.PHONY: install-minikube
install-minikube: install-minikube-mcp install-minikube-ui
	@echo "Both metric-mcp and metric-ui installed for Minikube"
	@echo ""
	@echo "Add these entries to your /etc/hosts file:"
	@echo "$$(minikube ip) ai-metrics-ui.local"
	@echo "$$(minikube ip) ai-metrics-mcp.local"
	@echo ""
	@echo "Then access the application at:"
	@echo "  UI: http://ai-metrics-ui.local"
	@echo "  API: http://ai-metrics-mcp.local"

.PHONY: uninstall-minikube
uninstall-minikube:
	@echo "Uninstalling Minikube deployments"
	@helm -n $(NAMESPACE) uninstall $(METRIC_MCP_RELEASE_NAME) || true
	@helm -n $(NAMESPACE) uninstall $(METRIC_UI_RELEASE_NAME) || true
	@echo "Minikube deployments uninstalled"

.PHONY: status-minikube
status-minikube:
	@echo "Checking Minikube deployment status..."
	@echo "Pods:"
	@kubectl get pods -n $(NAMESPACE) || true
	@echo ""
	@echo "Services:"
	@kubectl get svc -n $(NAMESPACE) || true
	@echo ""
	@echo "Ingress:"
	@kubectl get ingress -n $(NAMESPACE) || true
	@echo ""
	@echo "To check if ingress is working:"
	@echo "  curl -H 'Host: ai-metrics-ui.local' http://$$(minikube ip)/"
	@echo "  curl -H 'Host: ai-metrics-mcp.local' http://$$(minikube ip)/health"

# EC2-specific targets
.PHONY: install-ec2-mcp
install-ec2-mcp: namespace-minikube
	@echo "Installing metric-mcp for EC2"
	@if [ -z "$(PROMETHEUS_URL)" ] || [ -z "$(LLM_URL)" ]; then \
		echo "Error: PROMETHEUS_URL and LLM_URL must be set"; \
		echo "Example: make install-ec2-mcp PROMETHEUS_URL=http://prometheus.monitoring.svc.cluster.local:9090 LLM_URL=http://vllm-service.default.svc.cluster.local:8000"; \
		exit 1; \
	fi
	@helm upgrade --install $(METRIC_MCP_RELEASE_NAME) $(METRIC_MCP_CHART_PATH) \
		-n $(NAMESPACE) \
		-f $(METRIC_MCP_CHART_PATH)/values-ec2.yaml \
		--set config.prometheusUrl="$(PROMETHEUS_URL)" \
		--set llm.url="$(LLM_URL)" \
		$(if $(LLM_API_TOKEN),--set llm.apiToken="$(LLM_API_TOKEN)",)

.PHONY: install-ec2-ui
install-ec2-ui: namespace-minikube
	@echo "Installing metric-ui for EC2"
	@if [ -z "$(PROMETHEUS_URL)" ] || [ -z "$(LLM_URL)" ]; then \
		echo "Error: PROMETHEUS_URL and LLM_URL must be set"; \
		echo "Example: make install-ec2-ui PROMETHEUS_URL=http://prometheus.monitoring.svc.cluster.local:9090 LLM_URL=http://vllm-service.default.svc.cluster.local:8000"; \
		exit 1; \
	fi
	@helm upgrade --install $(METRIC_UI_RELEASE_NAME) $(METRIC_UI_CHART_PATH) \
		-n $(NAMESPACE) \
		-f $(METRIC_UI_CHART_PATH)/values-ec2.yaml \
		--set env.PROMETHEUS_URL="$(PROMETHEUS_URL)" \
		--set env.LLM_URL="$(LLM_URL)" \
		$(if $(LLM_API_TOKEN),--set env.LLM_API_TOKEN="$(LLM_API_TOKEN)",)

.PHONY: install-ec2
install-ec2: install-ec2-mcp install-ec2-ui configure-ec2-ingress
	@echo "Both metric-mcp and metric-ui installed for EC2"
	@echo ""
	@echo "Access the application directly via your EC2 public IP:"
	@echo "  UI: http://YOUR-EC2-PUBLIC-IP"
	@echo "  API: http://YOUR-EC2-PUBLIC-IP"
	@echo ""
	@echo "Get your EC2 public IP with:"
	@echo "  curl -s http://checkip.amazonaws.com/"

.PHONY: configure-ec2-ingress
configure-ec2-ingress:
	@echo "Configuring ingress for EC2 public IP access..."
	@kubectl patch ingress metric-mcp-ingress -n $(NAMESPACE) --type='json' -p='[{"op": "remove", "path": "/spec/rules/0/host"}]' || true
	@kubectl patch ingress ui-ingress -n $(NAMESPACE) --type='json' -p='[{"op": "remove", "path": "/spec/rules/0/host"}]' || true
	@echo "Ingress configured for EC2 access"